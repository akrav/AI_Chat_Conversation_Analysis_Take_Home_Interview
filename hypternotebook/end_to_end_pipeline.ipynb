{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# End-to-End Pipeline (Hypternotebook)\n",
        "\n",
        "This notebook runs the full pipeline on a 1,000-row English, non-toxic subset of WildChat:\n",
        "\n",
        "1. Load and filter data (English, non-toxic, limit=1000)\n",
        "2. Save cleaned JSONL\n",
        "3. Run BERTopic and save outputs\n",
        "4. Rule-based sentiment (VADER/TextBlob)\n",
        "5. LLM entity/intent/sentiment (assumes pre-computed JSONL present)\n",
        "6. Build canonical unified table (entity-level)\n",
        "7. Cluster entities and add labels\n",
        "8. Generate visuals into `reports/images`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root\n",
        "ROOT = Path('/Users/adam/Documents/GitHub/AI_Chat_Conversation_Analysis_Take_Home_Interview')\n",
        "DATA = ROOT / 'data'\n",
        "RAW = DATA / '01_raw'\n",
        "INTERIM = DATA / '02_interim'\n",
        "PROC = DATA / '03_processed'\n",
        "ANAL = DATA / '04_analysis'\n",
        "REPORTS = ROOT / 'reports'\n",
        "IMAGES = REPORTS / 'images'\n",
        "\n",
        "for p in [RAW, INTERIM, PROC, ANAL, IMAGES]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Dirs ready:', RAW, INTERIM, PROC, ANAL, IMAGES, sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Load and filter WildChat (English, non-toxic, 1000)\n",
        "from src.hf_loader import load_filter_to_dataframe, save_dataframe_jsonl\n",
        "\n",
        "limit = 1000\n",
        "raw_df = load_filter_to_dataframe(language='English', toxic=False, limit=limit)\n",
        "print('Loaded rows:', len(raw_df))\n",
        "raw_path = str(RAW / 'wildchat_english_notoxic_1000.jsonl')\n",
        "save_dataframe_jsonl(raw_df, raw_path)\n",
        "print('Saved:', raw_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Preprocess text (if your pipeline has a text_cleaning step)\n",
        "from src.preprocess import preprocess_and_save_from_raw\n",
        "cleaned_jsonl = preprocess_and_save_from_raw(raw_path, output_dir=str(INTERIM))\n",
        "print('Cleaned:', cleaned_jsonl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) BERTopic\n",
        "from src.bertopic_pipeline import load_docs_from_jsonl, run_bertopic, save_outputs\n",
        "\n",
        "# Load user-visible docs from cleaned jsonl\n",
        "identities, docs = load_docs_from_jsonl(cleaned_jsonl)\n",
        "print('Docs:', len(docs))\n",
        "\n",
        "model, topics, probs = run_bertopic(docs)\n",
        "paths = save_outputs(model, docs, topics, identities, output_dir=str(ROOT / 'reports'))\n",
        "print('BERTopic outputs:', paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Rule-based sentiment (VADER/TextBlob)\n",
        "from src.sentiment_rule_based import batch_process_subset\n",
        "\n",
        "subset_jsonl = cleaned_jsonl  # using same cleaned file (IDs used)\n",
        "rule_csv = str(ANAL / 'sentiment_rule_based_1000.csv')\n",
        "batch_process_subset(subset_jsonl, rule_csv)\n",
        "print('Rule-based sentiment saved:', rule_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) LLM analysis\n",
        "# Assumes LLM JSONL already exists at PROC / 'llm_analysis_results_1000_v4.jsonl'\n",
        "# If not, plug your LLM pipeline here to produce it (same schema used by _load_llm_df).\n",
        "llm_jsonl = str(PROC / 'llm_analysis_results_1000_v4.jsonl')\n",
        "print('LLM JSONL expected at:', llm_jsonl, '\\nExists:', os.path.exists(llm_jsonl))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Build canonical unified table (entity-level)\n",
        "from src.synthesis import build_unified_table\n",
        "\n",
        "unified_csv = str(ANAL / 'unified_table.csv')\n",
        "paths_bertopic = {\n",
        "    'doc_topics': str(ROOT / 'reports' / 'bertopic_doc_topics.csv'),\n",
        "    'topics': str(ROOT / 'reports' / 'bertopic_topics.csv'),\n",
        "}\n",
        "\n",
        "build_unified_table(\n",
        "    cleaned_jsonl_path=cleaned_jsonl,\n",
        "    doc_topics_csv_path=paths_bertopic['doc_topics'],\n",
        "    topics_csv_path=paths_bertopic['topics'],\n",
        "    rule_based_csv_path=rule_csv,\n",
        "    llm_jsonl_path=llm_jsonl,\n",
        "    output_csv_path=unified_csv,\n",
        ")\n",
        "print('Unified table:', unified_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Entity clustering with labels\n",
        "from src.entity_clustering import cluster_entities\n",
        "clustered = cluster_entities(unified_csv, output_csv_path=unified_csv, min_cluster_size=4)\n",
        "print('Clustered unified table:', clustered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Generate visuals\n",
        "from src.visualizations import generate_all\n",
        "results = generate_all(unified_csv, out_dir=str(IMAGES))\n",
        "results\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
